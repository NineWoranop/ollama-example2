server.port=8080
spring.application.name=ollama-example2
spring.main.web-application-type=servlet
spring.ai.retry.max-attempts=1
spring.ai.ollama.chat.enabled=true
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3.1
spring.ai.ollama.chat.options.mocomdel=llama3.1
spring.ai.ollama.chat.options.temperature=0.7
